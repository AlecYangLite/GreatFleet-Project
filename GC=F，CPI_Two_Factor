{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0EjBsGQcqOoX1QCQK1yIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlecYangLite/GreatFleet-Project/blob/main/GC%3DF%EF%BC%8CCPI_Two_Factor\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ï¸âƒ£ å¯¼å…¥ä¾èµ–\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional, Multiply\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from datetime import datetime\n",
        "\n",
        "# 2ï¸âƒ£ è‡ªåŠ¨ä» Yahoo Finance è·å– GC=F å’Œ TIP æ•°æ®\n",
        "start_date = \"2010-01-01\"\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "gc_data = yf.download('GC=F', start=start_date, end=today)[['Close']].rename(columns={'Close': 'GC=F'})\n",
        "tip_data = yf.download('TIP', start=start_date, end=today)[['Close']].rename(columns={'Close': 'TIP'})\n",
        "\n",
        "# åˆå¹¶\n",
        "factors = gc_data.join(tip_data, how='inner').dropna()\n",
        "print(\"âœ… æ”¶é›†åˆ°çš„æ•°æ®é¢„è§ˆ:\")\n",
        "print(factors.head())\n",
        "\n",
        "# 3ï¸âƒ£ ç¼ºå¤±å€¼æ£€æŸ¥å’Œæ¸…ç†å‡½æ•°\n",
        "def check_and_clean_nan(dataframe):\n",
        "    print(\"ğŸ“Š ç¼ºå¤±å€¼æ£€æŸ¥:\")\n",
        "    print(dataframe.isna().sum())\n",
        "    dataframe = dataframe.fillna(method='ffill').fillna(method='bfill').dropna()\n",
        "    return dataframe\n",
        "\n",
        "factors = check_and_clean_nan(factors)\n",
        "\n",
        "# 4ï¸âƒ£ å¤šåˆ—å½’ä¸€åŒ–\n",
        "scaler = MinMaxScaler()\n",
        "factors_scaled = scaler.fit_transform(factors)\n",
        "factors_scaled = pd.DataFrame(factors_scaled, index=factors.index, columns=factors.columns)\n",
        "\n",
        "# 5ï¸âƒ£ åˆ›å»ºè¾“å…¥åºåˆ—\n",
        "def create_multi_feature_sequences(data, time_step=90, forecast_horizon=5):\n",
        "    X, y = [], []\n",
        "    data_np = data.values\n",
        "    for i in range(len(data_np) - time_step - forecast_horizon):\n",
        "        X.append(data_np[i:i + time_step, :])\n",
        "        y.append(data_np[i + time_step:i + time_step + forecast_horizon, 0])  # é¢„æµ‹ GC=F\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_multi_feature_sequences(factors_scaled, time_step=90, forecast_horizon=5)\n",
        "\n",
        "# 6ï¸âƒ£ æ»‘åŠ¨çª—å£åˆ†å‰²\n",
        "def sliding_window_three_split(X, y, window_size=1000, slide_step=200, train_ratio=0.7, val_ratio=0.15):\n",
        "    windows = []\n",
        "    for start_idx in range(0, len(X) - window_size, slide_step):\n",
        "        end_idx = start_idx + window_size\n",
        "        X_window, y_window = X[start_idx:end_idx], y[start_idx:end_idx]\n",
        "\n",
        "        train_size = int(window_size * train_ratio)\n",
        "        val_size = int(window_size * val_ratio)\n",
        "\n",
        "        X_train, y_train = X_window[:train_size], y_window[:train_size]\n",
        "        X_val, y_val = X_window[train_size:train_size + val_size], y_window[train_size:train_size + val_size]\n",
        "        X_test, y_test = X_window[train_size + val_size:], y_window[train_size + val_size:]\n",
        "\n",
        "        windows.append({\n",
        "            'X_train': X_train, 'y_train': y_train,\n",
        "            'X_val': X_val, 'y_val': y_val,\n",
        "            'X_test': X_test, 'y_test': y_test\n",
        "        })\n",
        "    return windows\n",
        "\n",
        "windows = sliding_window_three_split(X, y, window_size=1000, slide_step=200)\n",
        "\n",
        "# 7ï¸âƒ£ Attention æ¨¡å—\n",
        "def attention_3d_block(inputs):\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Dense(input_dim, activation='softmax')(inputs)\n",
        "    outputs = Multiply()([inputs, a])\n",
        "    return outputs\n",
        "\n",
        "# 8ï¸âƒ£ æ„å»ºæ¨¡å‹\n",
        "def build_optimized_model(input_shape, forecast_horizon=5):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = attention_3d_block(x)\n",
        "    x = LSTM(64, return_sequences=False)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    output = Dense(forecast_horizon)(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.Huber())\n",
        "    return model\n",
        "\n",
        "# 9ï¸âƒ£ å¾ªç¯è®­ç»ƒè¯„ä¼°\n",
        "all_rmse_per_step = []\n",
        "all_mae_per_step = []\n",
        "\n",
        "for idx, window in enumerate(windows):\n",
        "    X_train, y_train = window['X_train'], window['y_train']\n",
        "    X_val, y_val = window['X_val'], window['y_val']\n",
        "    X_test, y_test = window['X_test'], window['y_test']\n",
        "\n",
        "    model = build_optimized_model(input_shape=(X_train.shape[1], X_train.shape[2]), forecast_horizon=5)\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, min_lr=1e-6)\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=60, batch_size=32,\n",
        "              validation_data=(X_val, y_val), verbose=1,\n",
        "              callbacks=[early_stop, reduce_lr])\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    rmse_steps = []\n",
        "    mae_steps = []\n",
        "    for step in range(5):\n",
        "        rmse_step = np.sqrt(mean_squared_error(y_test[:, step], predictions[:, step]))\n",
        "        mae_step = mean_absolute_error(y_test[:, step], predictions[:, step])\n",
        "        rmse_steps.append(rmse_step)\n",
        "        mae_steps.append(mae_step)\n",
        "\n",
        "    all_rmse_per_step.append(rmse_steps)\n",
        "    all_mae_per_step.append(mae_steps)\n",
        "\n",
        "    print(f\"âœ… ç¬¬ {idx+1} ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\")\n",
        "\n",
        "# ğŸ”Ÿ ç»˜åˆ¶è¯¯å·®è¶‹åŠ¿å›¾\n",
        "rmse_avg = np.mean(all_rmse_per_step, axis=0)\n",
        "mae_avg = np.mean(all_mae_per_step, axis=0)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1, 6), rmse_avg, marker='o', label='Average RMSE')\n",
        "plt.plot(range(1, 6), mae_avg, marker='s', label='Average MAE')\n",
        "plt.title('ç®€åŒ–ç‰ˆæ¨¡å‹ (GC=F + TIP) å¹³å‡è¯¯å·®è¶‹åŠ¿ (T+1 ~ T+5)')\n",
        "plt.xlabel('é¢„æµ‹æ­¥')\n",
        "plt.ylabel('è¯¯å·® (USD)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XypcuCibh2h9",
        "outputId": "ca8286d3-5cdb-4376-c4aa-e05ef5bf5f62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "<ipython-input-1-2595dc6cb097>:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  dataframe = dataframe.fillna(method='ffill').fillna(method='bfill').dropna()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ”¶é›†åˆ°çš„æ•°æ®é¢„è§ˆ:\n",
            "Price              GC=F        TIP\n",
            "Ticker             GC=F        TIP\n",
            "Date                              \n",
            "2010-01-04  1117.699951  71.852219\n",
            "2010-01-05  1118.099976  72.086708\n",
            "2010-01-06  1135.900024  71.900497\n",
            "2010-01-07  1133.099976  72.010857\n",
            "2010-01-08  1138.199951  72.162651\n",
            "ğŸ“Š ç¼ºå¤±å€¼æ£€æŸ¥:\n",
            "Price  Ticker\n",
            "GC=F   GC=F      0\n",
            "TIP    TIP       0\n",
            "dtype: int64\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - loss: 0.0841 - val_loss: 0.0500 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0487 - val_loss: 0.0446 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0347 - val_loss: 0.0347 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0264 - val_loss: 0.0329 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0194 - val_loss: 0.0214 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0138 - val_loss: 0.0162 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0101 - val_loss: 0.0156 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0073 - val_loss: 0.0139 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0053 - val_loss: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0038 - val_loss: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0020 - val_loss: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0015 - val_loss: 0.0058 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0012 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 8.5558e-04 - val_loss: 0.0049 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 6.4654e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 5.6171e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 5.1797e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 5.1196e-04 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 5.6584e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0818e-04\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.0746e-04 - val_loss: 0.0044 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.2756e-04 - val_loss: 0.0049 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 3.0893e-04 - val_loss: 0.0044 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2648e-04 - val_loss: 0.0046 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.0982e-04\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 3.0722e-04 - val_loss: 0.0046 - learning_rate: 5.0000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\n",
            "âœ… ç¬¬ 1 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 0.0837 - val_loss: 0.0524 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0486 - val_loss: 0.0443 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0364 - val_loss: 0.0364 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0271 - val_loss: 0.0272 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0195 - val_loss: 0.0145 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0131 - val_loss: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0095 - val_loss: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0067 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0036 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0027 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.7465e-04 - val_loss: 7.3061e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8032e-04 - val_loss: 5.7214e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.8098e-04 - val_loss: 3.6746e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2728e-04 - val_loss: 2.9008e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.6728e-04 - val_loss: 6.8512e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.3608e-04 - val_loss: 2.6078e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.3155e-04 - val_loss: 2.5503e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.8790e-04 - val_loss: 2.0183e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6160e-04 - val_loss: 1.8282e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4887e-04\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.4839e-04 - val_loss: 1.9539e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.3476e-04 - val_loss: 1.5570e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 3.3348e-04 - val_loss: 1.5761e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 3.3706e-04 - val_loss: 2.1153e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 2.7230e-04 - val_loss: 1.4672e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.9313e-04\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9484e-04 - val_loss: 1.4130e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.0626e-04 - val_loss: 1.5161e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.8693e-04 - val_loss: 1.5884e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.8843e-04 - val_loss: 1.5068e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.6104e-04\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6313e-04 - val_loss: 1.4151e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.9336e-04 - val_loss: 1.5029e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6918e-04 - val_loss: 1.5482e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7320e-04 - val_loss: 1.5386e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5101e-04\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.5416e-04 - val_loss: 1.4677e-04 - learning_rate: 1.2500e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
            "âœ… ç¬¬ 2 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 0.0784 - val_loss: 0.0544 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0494 - val_loss: 0.0440 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0366 - val_loss: 0.0282 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0252 - val_loss: 0.0190 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0178 - val_loss: 0.0137 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0126 - val_loss: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0090 - val_loss: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0042 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0026 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.8746e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.0080e-04 - val_loss: 7.7402e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.4535e-04 - val_loss: 4.4396e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8819e-04 - val_loss: 7.1221e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.9939e-04 - val_loss: 5.3980e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.3917e-04 - val_loss: 6.6467e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6407e-04\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5934e-04 - val_loss: 4.1070e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.8600e-04 - val_loss: 6.0472e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.0668e-04 - val_loss: 2.9214e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.9540e-04 - val_loss: 3.4765e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8487e-04 - val_loss: 4.2918e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 2.6201e-04 - val_loss: 3.5057e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0018e-04\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.9803e-04 - val_loss: 6.4983e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.6812e-04 - val_loss: 4.7198e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.4431e-04 - val_loss: 3.7705e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5233e-04 - val_loss: 3.6950e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7058e-04\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6869e-04 - val_loss: 3.9690e-04 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e1bd16fff60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/5\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 244ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e1bd16fff60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
            "âœ… ç¬¬ 3 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.0712 - val_loss: 0.0574 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0479 - val_loss: 0.0410 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0352 - val_loss: 0.0295 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0244 - val_loss: 0.0197 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0174 - val_loss: 0.0152 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0125 - val_loss: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0086 - val_loss: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0059 - val_loss: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0040 - val_loss: 0.0051 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0043 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0031 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.9305e-04 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6327e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.1780e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.3208e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.2226e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.0640e-04 - val_loss: 5.0840e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9947e-04 - val_loss: 9.1197e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.9626e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.4995e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5726e-04\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.5465e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.0044e-04 - val_loss: 9.3596e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.7338e-04 - val_loss: 9.0748e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4317e-04 - val_loss: 7.4218e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8655e-04\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.8277e-04 - val_loss: 7.9275e-04 - learning_rate: 5.0000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
            "âœ… ç¬¬ 4 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.0617 - val_loss: 0.0476 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0439 - val_loss: 0.0341 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0316 - val_loss: 0.0245 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0226 - val_loss: 0.0173 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0160 - val_loss: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0114 - val_loss: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.0062 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0056 - val_loss: 0.0043 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0031 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 9.1626e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.6491e-04 - val_loss: 7.9353e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 9.2149e-04 - val_loss: 8.4984e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 8.3506e-04 - val_loss: 9.4007e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 8.5643e-04 - val_loss: 9.6366e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.4201e-04 - val_loss: 6.6313e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.5496e-04 - val_loss: 7.7203e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 8.4719e-04 - val_loss: 8.8361e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.7885e-04 - val_loss: 7.8455e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1550e-04\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.2740e-04 - val_loss: 8.5844e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9300e-04 - val_loss: 7.6134e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4114e-04 - val_loss: 7.9754e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.9970e-04 - val_loss: 8.0795e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.9473e-04\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9381e-04 - val_loss: 6.8027e-04 - learning_rate: 5.0000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
            "âœ… ç¬¬ 5 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 0.0626 - val_loss: 0.0474 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0443 - val_loss: 0.0342 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0319 - val_loss: 0.0245 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0228 - val_loss: 0.0173 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0162 - val_loss: 0.0121 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0114 - val_loss: 0.0084 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0079 - val_loss: 0.0057 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0056 - val_loss: 0.0040 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0027 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 9.8296e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 8.5281e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 5.7480e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.8141e-04 - val_loss: 4.6012e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.9971e-04 - val_loss: 4.9642e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.7668e-04 - val_loss: 3.8316e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.5622e-04 - val_loss: 4.7725e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.4430e-04 - val_loss: 3.0835e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.4207e-04 - val_loss: 4.0874e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2899e-04 - val_loss: 3.9658e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.0287e-04 - val_loss: 3.2914e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.9889e-04\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 6.9899e-04 - val_loss: 4.7221e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 7.0819e-04 - val_loss: 3.5914e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 6.8966e-04 - val_loss: 4.0891e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.9326e-04 - val_loss: 3.5957e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.6939e-04\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.7208e-04 - val_loss: 3.5283e-04 - learning_rate: 5.0000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
            "âœ… ç¬¬ 6 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.0619 - val_loss: 0.0481 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0439 - val_loss: 0.0344 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0315 - val_loss: 0.0247 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0224 - val_loss: 0.0176 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0158 - val_loss: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0111 - val_loss: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0078 - val_loss: 0.0062 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0054 - val_loss: 0.0048 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 0.0033 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0027 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.7721e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3679e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.3162e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.6250e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.3396e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2955e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1021e-04\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.0981e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6487e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1125e-04 - val_loss: 0.0012 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.6942e-04 - val_loss: 0.0010 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9513e-04\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.9543e-04 - val_loss: 0.0011 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 6.8891e-04 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.0233e-04 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.9876e-04 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0894e-04\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 7.0710e-04 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.0076e-04 - val_loss: 0.0011 - learning_rate: 1.2500e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.0714e-04 - val_loss: 0.0011 - learning_rate: 1.2500e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.9331e-04 - val_loss: 0.0011 - learning_rate: 1.2500e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step\n",
            "âœ… ç¬¬ 7 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.0633 - val_loss: 0.0476 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0443 - val_loss: 0.0342 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0318 - val_loss: 0.0243 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0227 - val_loss: 0.0171 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0160 - val_loss: 0.0119 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0112 - val_loss: 0.0082 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0078 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0054 - val_loss: 0.0038 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0026 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 9.1219e-04 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 5.8413e-04 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 7.8138e-04 - val_loss: 4.4117e-04 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 6.5237e-04 - val_loss: 3.8540e-04 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.3750e-04 - val_loss: 3.4293e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.6730e-04 - val_loss: 2.8097e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9596e-04 - val_loss: 2.4946e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.2424e-04 - val_loss: 2.3238e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.2605e-04 - val_loss: 1.9932e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.2605e-04 - val_loss: 1.0895e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.0026e-04 - val_loss: 3.7725e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6794e-04 - val_loss: 2.2011e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9162e-04 - val_loss: 1.9099e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1180e-04\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1165e-04 - val_loss: 5.9281e-05 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6109e-04 - val_loss: 7.4059e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3484e-04 - val_loss: 6.1709e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.2310e-04 - val_loss: 9.6435e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2315e-04\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2375e-04 - val_loss: 4.8787e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3058e-04 - val_loss: 4.0694e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0822e-04 - val_loss: 5.7798e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.1003e-04 - val_loss: 5.9652e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1012e-04\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1005e-04 - val_loss: 4.3437e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 9.4273e-05 - val_loss: 4.7551e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 9.6501e-05 - val_loss: 4.6274e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 9.2453e-05 - val_loss: 4.9449e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.7129e-05\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 9.7030e-05 - val_loss: 4.2467e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.0465e-05 - val_loss: 4.2525e-05 - learning_rate: 6.2500e-05\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
            "âœ… ç¬¬ 8 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.0632 - val_loss: 0.0548 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0435 - val_loss: 0.0405 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0312 - val_loss: 0.0305 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0222 - val_loss: 0.0239 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0156 - val_loss: 0.0184 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0109 - val_loss: 0.0153 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0075 - val_loss: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0095 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 8.5098e-04 - val_loss: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8858e-04 - val_loss: 0.0073 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.2554e-04 - val_loss: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.7516e-04 - val_loss: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.1996e-04 - val_loss: 0.0070 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.5966e-04 - val_loss: 0.0071 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6756e-04\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.6642e-04 - val_loss: 0.0068 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.4780e-04 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.6024e-04 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.5642e-04 - val_loss: 0.0068 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.4739e-04\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.4658e-04 - val_loss: 0.0067 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7009e-04 - val_loss: 0.0069 - learning_rate: 2.5000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
            "âœ… ç¬¬ 9 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - loss: 0.0687 - val_loss: 0.0836 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0454 - val_loss: 0.0653 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0331 - val_loss: 0.0564 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0240 - val_loss: 0.0420 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0171 - val_loss: 0.0387 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0128 - val_loss: 0.0339 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0087 - val_loss: 0.0187 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0055 - val_loss: 0.0180 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0169 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0071 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8571e-04 - val_loss: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.5574e-04 - val_loss: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.2656e-04 - val_loss: 0.0061 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8819e-04 - val_loss: 0.0064 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.1666e-04 - val_loss: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.7913e-04 - val_loss: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 4.0265e-04 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 3.3995e-04 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 2.6527e-04 - val_loss: 0.0044 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.1098e-04 - val_loss: 0.0051 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.3858e-04 - val_loss: 0.0030 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.4194e-04 - val_loss: 0.0042 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2958e-04 - val_loss: 0.0037 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.7095e-04 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2308e-04 - val_loss: 0.0052 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.7894e-04 - val_loss: 0.0041 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.1628e-04 - val_loss: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.2525e-04\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.2100e-04 - val_loss: 0.0032 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.7453e-04 - val_loss: 0.0033 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6539e-04 - val_loss: 0.0035 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3177e-04 - val_loss: 0.0032 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4643e-04\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4671e-04 - val_loss: 0.0031 - learning_rate: 5.0000e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
            "âœ… ç¬¬ 10 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - loss: 0.0864 - val_loss: 0.0609 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0521 - val_loss: 0.0514 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0372 - val_loss: 0.0299 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0256 - val_loss: 0.0222 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0184 - val_loss: 0.0149 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0132 - val_loss: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0097 - val_loss: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0070 - val_loss: 0.0063 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - val_loss: 0.0037 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0033 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0031 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0031 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 9.8082e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.7245e-04 - val_loss: 6.4860e-04 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.1102e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.9272e-04 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.6084e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7527e-04\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7254e-04 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 5.1616e-04 - val_loss: 5.9774e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 4.2368e-04 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 4.2426e-04 - val_loss: 0.0013 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.9305e-04\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 3.9039e-04 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.4787e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.9628e-04 - val_loss: 0.0013 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6127e-04 - val_loss: 0.0014 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5632e-04\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.5694e-04 - val_loss: 0.0011 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.5037e-04 - val_loss: 0.0011 - learning_rate: 1.2500e-04\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
            "âœ… ç¬¬ 11 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.0967 - val_loss: 0.0642 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0529 - val_loss: 0.0473 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0412 - val_loss: 0.0321 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0309 - val_loss: 0.0223 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0214 - val_loss: 0.0167 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0158 - val_loss: 0.0124 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0118 - val_loss: 0.0094 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0089 - val_loss: 0.0071 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0066 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0040 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0015 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0014 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 7.1542e-04 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8781e-04 - val_loss: 7.6915e-04 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.5559e-04 - val_loss: 6.0057e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1570e-04 - val_loss: 7.9320e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.5176e-04 - val_loss: 3.8126e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.0899e-04 - val_loss: 4.0621e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.1679e-04 - val_loss: 3.3354e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.3524e-04 - val_loss: 3.2677e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.9134e-04 - val_loss: 2.6543e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5116e-04 - val_loss: 2.6040e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 5.1687e-04 - val_loss: 7.2925e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 5.8838e-04 - val_loss: 3.8866e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.3528e-04\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 4.3432e-04 - val_loss: 3.3894e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.8275e-04 - val_loss: 2.3825e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0181e-04 - val_loss: 2.5812e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6882e-04 - val_loss: 3.4730e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3002e-04\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3304e-04 - val_loss: 2.5458e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7450e-04 - val_loss: 3.7895e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3952e-04 - val_loss: 2.3797e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3896e-04 - val_loss: 2.4719e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1185e-04\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.1429e-04 - val_loss: 2.0231e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.3836e-04 - val_loss: 2.3505e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3062e-04 - val_loss: 1.9825e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.4836e-04 - val_loss: 2.4678e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9887e-04\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9896e-04 - val_loss: 2.6367e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.0561e-04 - val_loss: 2.2866e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.1495e-04 - val_loss: 2.0739e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.1260e-04 - val_loss: 2.1329e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2183e-04\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.2065e-04 - val_loss: 2.8490e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.2265e-04 - val_loss: 2.2245e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.4873e-04 - val_loss: 2.2145e-04 - learning_rate: 3.1250e-05\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step\n",
            "âœ… ç¬¬ 12 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.1164 - val_loss: 0.0564 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0519 - val_loss: 0.0417 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0402 - val_loss: 0.0332 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0319 - val_loss: 0.0262 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0253 - val_loss: 0.0208 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0198 - val_loss: 0.0161 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0152 - val_loss: 0.0132 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0119 - val_loss: 0.0101 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0093 - val_loss: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.0066 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061 - val_loss: 0.0054 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0044 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0041 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0021 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0021 - val_loss: 0.0017 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0016 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012 - val_loss: 9.9367e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.0779e-04 - val_loss: 7.7711e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 9.3627e-04 - val_loss: 6.8578e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.6301e-04 - val_loss: 6.8437e-04 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.7381e-04 - val_loss: 5.3260e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.1924e-04 - val_loss: 4.5178e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.8434e-04 - val_loss: 4.5186e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.3875e-04 - val_loss: 5.2756e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.8019e-04 - val_loss: 3.9138e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.8971e-04 - val_loss: 3.4651e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.5774e-04 - val_loss: 7.9350e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.6216e-04 - val_loss: 3.6349e-04 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6132e-04\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6098e-04 - val_loss: 3.9303e-04 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.5813e-04 - val_loss: 3.3103e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.2750e-04 - val_loss: 3.2445e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 3.3451e-04 - val_loss: 3.5480e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.4183e-04\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 3.4152e-04 - val_loss: 3.7997e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 3.1214e-04 - val_loss: 3.2956e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.9717e-04 - val_loss: 2.7687e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.1536e-04 - val_loss: 3.2603e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.7214e-04 - val_loss: 2.8207e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.0890e-04 - val_loss: 3.0643e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8848e-04\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.8859e-04 - val_loss: 2.9404e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.8171e-04 - val_loss: 2.8030e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9293e-04 - val_loss: 2.7201e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.6308e-04 - val_loss: 3.1169e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.8954e-04\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8876e-04 - val_loss: 2.8360e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.8430e-04 - val_loss: 3.2461e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.8257e-04 - val_loss: 2.7539e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.8165e-04 - val_loss: 2.7433e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.6915e-04\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6960e-04 - val_loss: 2.7243e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5297e-04 - val_loss: 2.8796e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.5856e-04 - val_loss: 2.8401e-04 - learning_rate: 3.1250e-05\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
            "âœ… ç¬¬ 13 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n",
            "Epoch 1/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.1229 - val_loss: 0.0610 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0508 - val_loss: 0.0442 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0383 - val_loss: 0.0338 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0296 - val_loss: 0.0255 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0229 - val_loss: 0.0197 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0174 - val_loss: 0.0154 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0135 - val_loss: 0.0129 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0105 - val_loss: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0082 - val_loss: 0.0073 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 - val_loss: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0046 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0035 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0040 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0037 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0049 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0026 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.0041 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 9.7068e-04 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0012 - val_loss: 8.9898e-04 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 8.7252e-04 - val_loss: 6.2912e-04 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.9615e-04 - val_loss: 5.7270e-04 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.4172e-04 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.4232e-04 - val_loss: 6.2038e-04 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.8354e-04 - val_loss: 5.1880e-04 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.6473e-04 - val_loss: 4.9230e-04 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.0167e-04 - val_loss: 4.8826e-04 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.5761e-04 - val_loss: 4.4572e-04 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.9768e-04 - val_loss: 3.7112e-04 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.9602e-04 - val_loss: 3.6924e-04 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.1204e-04 - val_loss: 3.7735e-04 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3919e-04 - val_loss: 3.4759e-04 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7737e-04\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.7785e-04 - val_loss: 3.3641e-04 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3986e-04 - val_loss: 2.8921e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.6510e-04 - val_loss: 2.7151e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8858e-04 - val_loss: 3.0275e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.7312e-04 - val_loss: 2.6144e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.8279e-04 - val_loss: 2.9186e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.7163e-04 - val_loss: 3.3777e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 2.4965e-04 - val_loss: 2.3466e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.3315e-04\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 2.3359e-04 - val_loss: 2.8280e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5244e-04 - val_loss: 2.3714e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3078e-04 - val_loss: 2.7160e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.2721e-04 - val_loss: 2.4662e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.2319e-04\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2378e-04 - val_loss: 2.5099e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.1537e-04 - val_loss: 2.2716e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2976e-04 - val_loss: 2.2116e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.1668e-04 - val_loss: 2.3053e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m21/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.2126e-04\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.2121e-04 - val_loss: 2.0801e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9820e-04 - val_loss: 2.0611e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1399e-04 - val_loss: 2.2185e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.1016e-04 - val_loss: 2.0489e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m19/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.1651e-04\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1626e-04 - val_loss: 2.0885e-04 - learning_rate: 6.2500e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.0567e-04 - val_loss: 2.0993e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1529e-04 - val_loss: 2.1670e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.0590e-04 - val_loss: 2.2928e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m20/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.3238e-04\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.2948e-04 - val_loss: 2.2166e-04 - learning_rate: 3.1250e-05\n",
            "Epoch 58/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9486e-04 - val_loss: 2.2653e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 59/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.0934e-04 - val_loss: 2.2316e-04 - learning_rate: 1.5625e-05\n",
            "Epoch 60/60\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.9891e-04 - val_loss: 2.2318e-04 - learning_rate: 1.5625e-05\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step\n",
            "âœ… ç¬¬ 14 ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 35823 (\\N{CJK UNIFIED IDEOGRAPH-8BEF}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 24046 (\\N{CJK UNIFIED IDEOGRAPH-5DEE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 31616 (\\N{CJK UNIFIED IDEOGRAPH-7B80}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 29256 (\\N{CJK UNIFIED IDEOGRAPH-7248}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22343 (\\N{CJK UNIFIED IDEOGRAPH-5747}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 36235 (\\N{CJK UNIFIED IDEOGRAPH-8D8B}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 21183 (\\N{CJK UNIFIED IDEOGRAPH-52BF}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 39044 (\\N{CJK UNIFIED IDEOGRAPH-9884}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27979 (\\N{CJK UNIFIED IDEOGRAPH-6D4B}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27493 (\\N{CJK UNIFIED IDEOGRAPH-6B65}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# åå½’ä¸€åŒ–å‡½æ•°ï¼ˆåªé’ˆå¯¹ GC=F ç¬¬ä¸€åˆ—ï¼‰\n",
        "def inverse_transform_gc_values(scaled_values, scaler_obj):\n",
        "    dummy = np.zeros((len(scaled_values), scaler_obj.scale_.shape[0]))\n",
        "    dummy[:, 0] = scaled_values  # åªæ”¾å› GC=F åˆ—\n",
        "    inversed = scaler_obj.inverse_transform(dummy)[:, 0]\n",
        "    return inversed\n",
        "\n",
        "# å¾ªç¯è®¡ç®—åå½’ä¸€åŒ–è¯¯å·®\n",
        "total_rmse_real = []\n",
        "total_mae_real = []\n",
        "\n",
        "for idx, window in enumerate(windows):\n",
        "    X_test, y_test = window['X_test'], window['y_test']\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    rmse_real_steps = []\n",
        "    mae_real_steps = []\n",
        "\n",
        "    for step in range(5):\n",
        "        y_true_rescaled = inverse_transform_gc_values(y_test[:, step], scaler)\n",
        "        y_pred_rescaled = inverse_transform_gc_values(predictions[:, step], scaler)\n",
        "\n",
        "        rmse_rescaled = np.sqrt(mean_squared_error(y_true_rescaled, y_pred_rescaled))\n",
        "        mae_rescaled = mean_absolute_error(y_true_rescaled, y_pred_rescaled)\n",
        "\n",
        "        rmse_real_steps.append(rmse_rescaled)\n",
        "        mae_real_steps.append(mae_rescaled)\n",
        "\n",
        "    total_rmse_real.append(rmse_real_steps)\n",
        "    total_mae_real.append(mae_real_steps)\n",
        "\n",
        "# æ‰“å°åå½’ä¸€åŒ–åçš„å¹³å‡è¯¯å·®ï¼ˆå•ä½ USDï¼‰\n",
        "avg_rmse_real = np.mean(total_rmse_real, axis=0)\n",
        "avg_mae_real = np.mean(total_mae_real, axis=0)\n",
        "\n",
        "print(\"ğŸ“Š åå½’ä¸€åŒ–åçš„çœŸå®è¯¯å·® (å•ä½ï¼šUSD):\")\n",
        "for step in range(5):\n",
        "    print(f\"T+{step+1}: RMSE = {avg_rmse_real[step]:.2f}, MAE = {avg_mae_real[step]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZJ2RSCjtr3D",
        "outputId": "17f6af52-9e61-473a-e907-7e2dd361566a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "ğŸ“Š åå½’ä¸€åŒ–åçš„çœŸå®è¯¯å·® (å•ä½ï¼šUSD):\n",
            "T+1: RMSE = 246.34, MAE = 239.45\n",
            "T+2: RMSE = 243.98, MAE = 236.96\n",
            "T+3: RMSE = 247.16, MAE = 239.95\n",
            "T+4: RMSE = 255.89, MAE = 248.53\n",
            "T+5: RMSE = 256.31, MAE = 248.81\n"
          ]
        }
      ]
    }
  ]
}